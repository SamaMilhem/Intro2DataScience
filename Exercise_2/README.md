# ğŸ“–âœ¨ **Text Analysis and Processing of *Little Women*** ğŸ“š

This project applies various **text processing techniques** to analyze the book *Little Women* using Python. The main features include **tokenization**, **stopword removal**, **stemming**, **word frequency distribution (Zipfâ€™s Law)**, **part-of-speech tagging**, and **word cloud generation**. Additionally, a pattern-matching method is included for identifying repeated words followed by punctuation.

---

## ğŸŒŸ **Key Features**

### 1ï¸âƒ£ **Text Tokenization** ğŸ” 

The `text_tokenize(paragraphs)` function breaks down the input text into smaller units (tokens), removes punctuation, and splits hyphenated words.

- **Function:** `text_tokenize(paragraphs)`
    - **Input:** `paragraphs` â€“ A list of paragraphs (strings) to tokenize.
    - **Output:** A list of tokenized words.

### 2ï¸âƒ£ **Stopword Removal** ğŸ›‘

The `remove_stopwords(paragraphs)` function removes common, non-informative words (stopwords), using both a custom list and **NLTK's** stopwords.

- **Function:** `remove_stopwords(paragraphs)`
    - **Input:** `paragraphs` â€“ A list of paragraphs (strings) to process.
    - **Output:** A list of tokenized words with stopwords removed.

### 3ï¸âƒ£ **Word Frequency Distribution (Zipfâ€™s Law)** ğŸ“‰

The `counting_terms(tokens, title, save_path)` function visualizes the log rank vs. log frequency distribution of terms, demonstrating **Zipf's Law**.

- **Function:** `counting_terms(tokens, title, save_path)`
    - **Input:** 
        - `tokens` â€“ A list of tokenized words.
        - `title` â€“ Title for the plot.
        - `save_path` â€“ Optional, a path to save the plot image.
    - **Output:** A plot showing Zipf's Law and a sorted list of token frequencies.

### 4ï¸âƒ£ **Stemming Tokens** ğŸŒ¿

The `stemming_tokens(tokens)` function reduces words to their root forms using the **Porter Stemmer**.

- **Function:** `stemming_tokens(tokens)`
    - **Input:** `tokens` â€“ A list of tokenized words.
    - **Output:** A list of stemmed words.

### 5ï¸âƒ£ **Part-of-Speech (POS) Tagging** ğŸ·ï¸

The `pos_tag(tokens)` function assigns part-of-speech tags to the tokenized words, identifying nouns, verbs, adjectives, and more.

- **Function:** `pos_tag(tokens)`
    - **Input:** `tokens` â€“ A list of tokenized words.
    - **Output:** A list of tuples, each containing a word and its corresponding POS tag.

### 6ï¸âƒ£ **Word Cloud Generation** â˜ï¸

The `tag_clouds_creation(tagged, save_path)` function creates a **word cloud** from proper nouns (e.g., names of people or places) based on part-of-speech tagging.

- **Function:** `tag_clouds_creation(tagged, save_path)`
    - **Input:**
        - `tagged` â€“ A list of POS-tagged tokens.
        - `save_path` â€“ Optional, a path to save the word cloud image.
    - **Output:** Displays a word cloud and saves it if a path is provided.

---

## ğŸš€ **Usage**

1. **Load the text** (HTML format) and tokenize it using `text_tokenize()`:
    ```python
    tokens = text_tokenize(paragraphs)
    ```
2. **Remove stopwords**:
    ```python
    tokens_clean = remove_stopwords(tokens)
    ```
3. **Plot and analyze** word frequency distribution using **Zipfâ€™s Law**:
    ```python
    counting_terms(tokens_clean, "Word Frequency in Little Women", save_path="word_freq.png")
    ```
4. **Stem the tokens**:
    ```python
    stemmed_tokens = stemming_tokens(tokens_clean)
    ```
5. **Tag for part-of-speech**:
    ```python
    tagged_tokens = pos_tag(stemmed_tokens)
    ```
6. **Generate a word cloud** of proper nouns:
    ```python
    tag_clouds_creation(tagged_tokens, save_path="wordcloud.png")
    ```

